{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a9074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습1\n",
    "# 뉴스의 링크를 넘기면 해당 뉴스의 댓글 통계 정보를 딕셔너리로 반환하는 함수 만들기\n",
    "\n",
    "# {'남자' : '68%', '여자' : '32%', '10대' : '1%', '20대' : '4%', '30대' : '17%',\n",
    "#  '40대' : '45%', '50대' : '26%', '60대' : '8%' }\n",
    "\n",
    "# 문제를 나누기\n",
    "# 뉴스의 링크를 넘긴다\n",
    "# 해당뉴스의 댓글 통계 정보를 추출해서 딕셔너리로 받는다.\n",
    "# joson으로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef68f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 일단 페이지 1개를 설정\n",
    "# 이페이지가 정적인지 동적인지 알수 없다.\n",
    "# 정적 : request, 동적: seliumn을 씀\n",
    "# request가 더 빠름.\n",
    "# 동적이면 안뜨것지\n",
    "# 우선 request로 테스트!\n",
    "\n",
    "import requests\n",
    "\n",
    "## 파이썬이 브라우저인 척 하기 위함\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get('https://n.news.naver.com/article/087/0000995105?ntype=RANKING', headers=headers)\n",
    "\n",
    "test_news_page=r.text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(test_news_page, 'html.parser')\n",
    "# soup으로 r page parsing 되서 옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2e70bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'남자' : '68%', '여자' : '32%', '10대' : '1%', '20대' : '4%', '30대' : '17%',\n",
    "#  '40대' : '45%', '50대' : '26%', '60대' : '8%' }\n",
    "test=soup.select(\".u_cbox_chart_cont\")\n",
    "test #[] : request로 가져왔을때 이러면 이부분은 동적\n",
    "# 페이지는 정적부분 + 동적 부분으로 이루어 지기 때문에\n",
    "# 즉 여기서 이걸 가져오려면 request+BequtifulSoup으로 안됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44cffe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenuim을 사용해보자!\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "import time\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_argument('headless')\n",
    "service = ChromeService(executable_path='C:/Users/admin/psm/chromedriver-win64/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "driver.get('https://n.news.naver.com/article/087/0000995105?ntype=RANKING')\n",
    "time.sleep(10)\n",
    "chartDiv=driver.find_element(By.CLASS_NAME,'u_cbox_chart_cont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9dc9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "innerChart=chartDiv.find_element(By.CLASS_NAME,'u_cbox_chart_cont_inner')\n",
    "perNum=innerChart.find_elements(By.CLASS_NAME,'u_cbox_chart_per')\n",
    "\n",
    "# innerChart.text #'76%\\n남자\\n24%\\n여자\\n0%\\n10대\\n5%\\n20대\\n27%\\n30대\\n46%\\n40대\\n17%\\n50대\\n4%\\n60대\n",
    "names=innerChart.find_elements(By.CLASS_NAME,'u_cbox_chart_cnt')\n",
    "\n",
    "\n",
    "# %들의 class 이름이 u_cbox_chart_per 임.\n",
    "# name들의 class 이름이 u_cbox_chart_cnt 임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8ecbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20대'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innerChart.text\n",
    "names[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be56d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 그럼 이페이지의 class=u_cbox_chart_per, class=u_cbox_chart_cnt 를 dic 하나에 모아서 저장되는지 시도!\n",
    "# result_dict = {}\n",
    "\n",
    "# # 요소를 반복하면서 데이터를 딕셔너리에 저장\n",
    "# for i in range(min(len(perNum), len(names))):\n",
    "#     key = names[i].text\n",
    "#     value = perNum[i].text\n",
    "#     result_dict[key] = value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b1ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824645b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지를 함수로 만들기\n",
    "def article_value(dirver,href):\n",
    "    chartDiv=driver.find_element(By.CLASS_NAME,'u_cbox_chart_cont')\n",
    "    innerChart=chartDiv.find_element(By.CLASS_NAME,'u_cbox_chart_cont_inner')\n",
    "    perNum=innerChart.find_elements(By.CLASS_NAME,'u_cbox_chart_per')\n",
    "    names=innerChart.find_elements(By.CLASS_NAME,'u_cbox_chart_cnt')\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    for i in range(min(len(perNum), len(names))):\n",
    "        key = names[i].text\n",
    "        value = perNum[i].text\n",
    "        result_dict[key] = value\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이걸 페이지 마다 하면 됨. 근데 모든 페이지마다 우선 하면 힘드니까 페이지도 하나하나 짜르자\n",
    "# 1단계 00언론사의 랭킹 페이지(20개 기사)의 댓글만 가져와서 저장\n",
    "\n",
    "#  00 언론사의 기사 링크가 있는 걸 다 가져올수 있는지 테스트\n",
    "\n",
    "# 1. 00 언론사의 기사 랭킹페이지는 정적인가 동적인가 판단\n",
    "# request로 때린다.\n",
    "\n",
    "import requests\n",
    "\n",
    "## 파이썬이 브라우저인 척 하기 위함\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get('https://media.naver.com/press/028/ranking?type=popular', headers=headers)\n",
    "\n",
    "test_rankingnews_page=r.text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(test_rankingnews_page, 'html.parser')\n",
    "# soup으로 r page parsing 되서 옴\n",
    "thumb_list=soup.select(\".as_thumb\")\n",
    "\n",
    "# 아? 위는 안되고 아래는 되는 이유? thumb_list가 ResultSet 객체라서 라는데 이해 안됨.\n",
    "# a_tags = thumb_list.selects('a')\n",
    "a_tags = [thumb.select_one('a') for thumb in thumb_list]\n",
    "\n",
    "Id_dic={}\n",
    "\n",
    "for a_tag in a_tags:\n",
    "    href = a_tag.get('href')\n",
    "    \n",
    "    article_value(href)\n",
    "    \n",
    "    split=href.split('?')\n",
    "    split=split[0].split('/')\n",
    "    Id=split[-1]\n",
    "    \n",
    "    for i in range(len(result_dict)):\n",
    "        key = Id\n",
    "        value = result_dict\n",
    "        Id_dic[key] = value\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Id_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지를 하나의 함수로 만들기\n",
    "def ranking_news(driver, href):\n",
    "#     thumb_list=driver.select(\".as_thumb\")\n",
    "    thumb_list = driver.find_elements(By.CLASS_NAME, \"as_thumb\")\n",
    "\n",
    "    a_tags = [thumb.select_one('a') for thumb in thumb_list]\n",
    "\n",
    "    Id_dic={}\n",
    "    \n",
    "    for a_tag in a_tags:\n",
    "#         href = a_tag.get('href')\n",
    "\n",
    "        article_value(driver, href)  # 함수 호출\n",
    "\n",
    "#         article_value(href) #함수 호출\n",
    "\n",
    "        split=href.split('?')\n",
    "        split=split[0].split('/')\n",
    "        Id=split[-1]\n",
    "\n",
    "        for i in range(len(result_dict)):\n",
    "            key = Id\n",
    "            value = result_dict\n",
    "            Id_dic[key] = value\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Id_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d83e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "## 파이썬이 브라우저인 척 하기 위함\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get('https://news.naver.com/main/ranking/popularDay.naver', headers=headers)\n",
    "\n",
    "test_rankingnews_page=r.text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(test_rankingnews_page, 'html.parser')\n",
    "# soup으로 r page parsing 되서 옴\n",
    "box_list=soup.select(\".rankingnews_box\")\n",
    "\n",
    "# 각 요소에서 'a' 태그 선택 : 사이트 링크\n",
    "a_tags = [box.select_one('a') for box in box_list]\n",
    "\n",
    "#key로 들어갈 언론사 이름 추출\n",
    "name_key = [a.select_one('.rankingnews_name').text for a in a_tags]\n",
    "\n",
    "company_dic={}\n",
    "\n",
    "for a_tage in a_tags:\n",
    "    href = a_tag.get('href')\n",
    "    \n",
    "    ranking_news(driver, href)\n",
    "    \n",
    "    for i in range(len(result_dict)):\n",
    "        key = Id\n",
    "        value = result_dict\n",
    "        company_dic[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32967fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(company_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5f6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8302936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2단계 랭킹페이지에서 페이지들의 링크로 모든 언론사 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 페이지 하나에서 댓글을 추출할수 있는지 경로 확인\n",
    "\n",
    "# 3. json 저장(test)\n",
    "\n",
    "# 4. 방법을 동일하게 해서 반복을 돌린다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습2\n",
    "# SBS, MBC, KBS 세 언론사의 모든 랭킹뉴스 통계 정보를 json 파일로 저장해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a95277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습3\n",
    "# 각 뉴스의 댓글 수를 스크랩해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습4\n",
    "# 수집한 뉴스데이터를 이용해 다음 문제를 풀어주세요.\n",
    "## 1. 20대가 가장 많이 본(댓글을 많이 작성한) 뉴스\n",
    "\n",
    "## 2. 가장 댓글이 적은 뉴스의 댓글 수와 뉴스 링크, 뉴스 번호\n",
    "\n",
    "## 3. 언론사 번호와 언론사 이름으로 구성된 데이터 프레임을 만들고 merge를 이용해 각 뉴스에 언론사 이름을 붙여주기\n",
    "\n",
    "## 4. 각 언론사별 평균 댓글 수(댓글 수로 내림차순 정렬)\n",
    "\n",
    "## 5. 여성의 댓글수가 가장 많은 언론사\n",
    "\n",
    "## 6. 각 언론사 별 댓글을 많이 작성한 연령대 top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a4f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84b2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
